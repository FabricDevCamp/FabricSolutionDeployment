{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "a365ComputeOptions": null,
    "sessionKeepAliveTimeout": 0,
    "dependencies": {
      "lakehouse": {
        "default_lakehouse": "{LAKEHOUSE_ID}",
        "default_lakehouse_name": "{LAKEHOUSE_NAME}",
        "default_lakehouse_workspace_id": "{WORKSPACE_ID}",
        "known_lakehouses": [
          {
            "id": "{LAKEHOUSE_ID}"
          }
        ]
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# copy CSV files to lakehouse to load data into bronze layer \r\n",
        "import requests\r\n",
        "\r\n",
        "csv_base_url = \"https://fabricdevcamp.blob.core.windows.net/sampledata/ProductSales/Dev/\"\r\n",
        "\r\n",
        "csv_files = { \"Customers.csv\", \"Products.csv\", \"Invoices.csv\", \"InvoiceDetails.csv\" }\r\n",
        "\r\n",
        "folder_path = \"Files/sales-data/\"\r\n",
        "\r\n",
        "for csv_file in csv_files:\r\n",
        "    csv_file_path = csv_base_url + csv_file\r\n",
        "    with requests.get(csv_file_path) as response:\r\n",
        "        csv_content = response.content.decode('utf-8-sig')\r\n",
        "        mssparkutils.fs.put(folder_path + csv_file, csv_content, True)\r\n",
        "        print(csv_file + \" copied to Lakehouse file in OneLake\")"
      ],
      "outputs": [
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create products table for silver layer\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType\r\n",
        "\r\n",
        "# create schema for products table using StructType and StructField \r\n",
        "schema_products = StructType([\r\n",
        "    StructField(\"ProductId\", LongType() ),\r\n",
        "    StructField(\"Product\", StringType() ),\r\n",
        "    StructField(\"Category\", StringType() )\r\n",
        "])\r\n",
        "\r\n",
        "# Load CSV file into Spark DataFrame and validate data using schema\r\n",
        "df_products = (\r\n",
        "    spark.read.format(\"csv\")\r\n",
        "         .option(\"header\",\"true\")\r\n",
        "         .schema(schema_products)\r\n",
        "         .load(\"Files/sales-data/Products.csv\")\r\n",
        ")\r\n",
        "\r\n",
        "# save DataFrame as lakehouse table in Delta format\r\n",
        "( df_products.write\r\n",
        "             .mode(\"overwrite\")\r\n",
        "             .option(\"overwriteSchema\", \"True\")\r\n",
        "             .format(\"delta\")\r\n",
        "             .save(\"Tables/silver_products\")\r\n",
        ")\r\n",
        "\r\n",
        "# display table schema and data\r\n",
        "df_products.printSchema()\r\n",
        "df_products.show()"
      ],
      "outputs": [
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create customers table for silver layer\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, DateType\r\n",
        "\r\n",
        "# create schema for customers table using StructType and StructField \r\n",
        "schema_customers = StructType([\r\n",
        "    StructField(\"CustomerId\", LongType() ),\r\n",
        "    StructField(\"FirstName\", StringType() ),\r\n",
        "    StructField(\"LastName\", StringType() ),\r\n",
        "    StructField(\"Country\", StringType() ),\r\n",
        "    StructField(\"City\", StringType() ),\r\n",
        "    StructField(\"DOB\", DateType() ),\r\n",
        "])\r\n",
        "\r\n",
        "# Load CSV file into Spark DataFrame with schema and support to infer dates\r\n",
        "df_customers = (\r\n",
        "    spark.read.format(\"csv\")\r\n",
        "         .option(\"header\",\"true\")\r\n",
        "         .schema(schema_customers)\r\n",
        "         .option(\"dateFormat\", \"MM/dd/yyyy\")\r\n",
        "         .option(\"inferSchema\", \"true\")\r\n",
        "         .load(\"Files/sales-data/Customers.csv\")\r\n",
        ")\r\n",
        "\r\n",
        "# save DataFrame as lakehouse table in Delta format\r\n",
        "( df_customers.write\r\n",
        "              .mode(\"overwrite\")\r\n",
        "              .option(\"overwriteSchema\", \"True\")\r\n",
        "              .format(\"delta\")\r\n",
        "              .save(\"Tables/silver_customers\")\r\n",
        ")\r\n",
        "\r\n",
        "# display table schema and data\r\n",
        "df_customers.printSchema()\r\n",
        "df_customers.show()"
      ],
      "outputs": [
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create invoices table for silver layer\r\n",
        "from pyspark.sql.types import StructType, StructField, LongType, FloatType, DateType\r\n",
        "\r\n",
        "# create schema for invoices table using StructType and StructField \r\n",
        "schema_invoices = StructType([\r\n",
        "    StructField(\"InvoiceId\", LongType() ),\r\n",
        "    StructField(\"Date\", DateType() ),\r\n",
        "    StructField(\"TotalSalesAmount\", FloatType() ),\r\n",
        "    StructField(\"CustomerId\", LongType() )\r\n",
        "])\r\n",
        "\r\n",
        "# Load CSV file into Spark DataFrame with schema and support to infer dates\r\n",
        "df_invoices = (\r\n",
        "    spark.read.format(\"csv\")\r\n",
        "         .option(\"header\",\"true\")\r\n",
        "         .schema(schema_invoices)\r\n",
        "         .option(\"dateFormat\", \"MM/dd/yyyy\")\r\n",
        "         .option(\"inferSchema\", \"true\") \r\n",
        "         .load(\"Files/sales-data/Invoices.csv\")\r\n",
        ")\r\n",
        "\r\n",
        "# save DataFrame as lakehouse table in Delta format\r\n",
        "( df_invoices.write\r\n",
        "             .mode(\"overwrite\")\r\n",
        "             .option(\"overwriteSchema\", \"True\")\r\n",
        "             .format(\"delta\")\r\n",
        "             .save(\"Tables/silver_invoices\")\r\n",
        ")\r\n",
        "\r\n",
        "# display table schema and data\r\n",
        "df_invoices.printSchema()\r\n",
        "df_invoices.show()"
      ],
      "outputs": [
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create invoice_details table for silver layer\r\n",
        "from pyspark.sql.types import StructType, StructField, LongType, FloatType\r\n",
        "\r\n",
        "# create schema for invoice_details table using StructType and StructField \r\n",
        "schema_invoice_details = StructType([\r\n",
        "    StructField(\"Id\", LongType() ),\r\n",
        "    StructField(\"Quantity\", LongType() ),\r\n",
        "    StructField(\"SalesAmount\", FloatType() ),\r\n",
        "    StructField(\"InvoiceId\", LongType() ),\r\n",
        "    StructField(\"ProductId\", LongType() )\r\n",
        "])\r\n",
        "\r\n",
        "# Load CSV file into Spark DataFrame and validate data using schema\r\n",
        "df_invoice_details = (\r\n",
        "    spark.read.format(\"csv\")\r\n",
        "         .option(\"header\",\"true\")\r\n",
        "         .schema(schema_invoice_details)\r\n",
        "         .load(\"Files/sales-data/InvoiceDetails.csv\")\r\n",
        ")\r\n",
        "\r\n",
        "# save DataFrame as lakehouse table in Delta format\r\n",
        "( df_invoice_details.write\r\n",
        "                    .mode(\"overwrite\")\r\n",
        "                    .option(\"overwriteSchema\", \"True\")\r\n",
        "                    .format(\"delta\")\r\n",
        "                    .save(\"Tables/silver_invoice_details\")\r\n",
        ")\r\n",
        "\r\n",
        "# display table schema and data\r\n",
        "df_invoice_details.printSchema()\r\n",
        "df_invoice_details.show()"
      ],
      "outputs": [
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create products table for gold layer\r\n",
        "\r\n",
        "# load DataFrame from silver layer table\r\n",
        "df_gold_products = (\r\n",
        "    spark.read\r\n",
        "         .format(\"delta\")\r\n",
        "         .load(\"Tables/silver_products\")\r\n",
        ")\r\n",
        "\r\n",
        "# write DataFrame to new gold layer table \r\n",
        "( df_gold_products.write\r\n",
        "                  .mode(\"overwrite\")\r\n",
        "                  .option(\"overwriteSchema\", \"True\")\r\n",
        "                  .format(\"delta\")\r\n",
        "                  .save(\"Tables/products\")\r\n",
        ")\r\n",
        "\r\n",
        "# display table schema and data\r\n",
        "df_gold_products.printSchema()\r\n",
        "df_gold_products.show()"
      ],
      "outputs": [
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create customers table for gold layer\r\n",
        "from pyspark.sql.functions import concat_ws, floor, datediff, current_date, col\r\n",
        "\r\n",
        "# load DataFrame from silver layer table and perform transforms\r\n",
        "df_gold_customers = (\r\n",
        "    spark.read\r\n",
        "         .format(\"delta\")\r\n",
        "         .load(\"Tables/silver_customers\")\r\n",
        "         .withColumnRenamed(\"City\", \"CityName\")\r\n",
        "         .withColumn(\"City\", concat_ws(', ', col('CityName'), col('Country')) )\r\n",
        "         .withColumn(\"Customer\", concat_ws(' ', col('FirstName'), col('LastName')) )\r\n",
        "         .withColumn(\"Age\",( floor( datediff( current_date(), col(\"DOB\") )/365.25) ))   \r\n",
        "         .drop('FirstName', 'LastName')\r\n",
        ")\r\n",
        "\r\n",
        "# write DataFrame to new gold layer table \r\n",
        "( df_gold_customers.write\r\n",
        "                   .mode(\"overwrite\")\r\n",
        "                   .option(\"overwriteSchema\", \"True\")\r\n",
        "                   .format(\"delta\")\r\n",
        "                   .save(\"Tables/customers\")\r\n",
        ")\r\n",
        "\r\n",
        "# display table schema and data\r\n",
        "df_gold_customers.printSchema()\r\n",
        "df_gold_customers.show()"
      ],
      "outputs": [
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create sales table for gold layer\r\n",
        "from pyspark.sql.functions import col, desc, concat, lit, floor, datediff\r\n",
        "from pyspark.sql.functions import date_format, to_date, current_date, year, month, dayofmonth\r\n",
        "\r\n",
        "# load DataFrames using invoices table and invoice_details table from silver layer\r\n",
        "df_silver_invoices = spark.read.format(\"delta\").load(\"Tables/silver_invoices\")\r\n",
        "df_silver_invoice_details = spark.read.format(\"delta\").load(\"Tables/silver_invoice_details\")\r\n",
        "\r\n",
        "# perform join to merge columns from both DataFrames and transform data \r\n",
        "df_gold_sales = (\r\n",
        "    df_silver_invoice_details\r\n",
        "        .join(df_silver_invoices, \r\n",
        "              df_silver_invoice_details['InvoiceId'] == df_silver_invoices['InvoiceId'])\r\n",
        "        .withColumnRenamed('SalesAmount', 'Sales')\r\n",
        "        .withColumn(\"DateKey\", (year(col('Date'))*10000) + \r\n",
        "                               (month(col('Date'))*100) + \r\n",
        "                               (dayofmonth(col('Date')))   )\r\n",
        "        .drop('InvoiceId', 'TotalSalesAmount', 'InvoiceId', 'Id')\r\n",
        "        .select('Date', \"DateKey\", \"CustomerId\", \"ProductId\", \"Sales\", \"Quantity\")\r\n",
        ")\r\n",
        "\r\n",
        "# write DataFrame to new gold layer table \r\n",
        "( df_gold_sales.write\r\n",
        "               .mode(\"overwrite\")\r\n",
        "               .option(\"overwriteSchema\", \"True\")\r\n",
        "               .format(\"delta\")\r\n",
        "               .save(\"Tables/sales\")\r\n",
        ")\r\n",
        "\r\n",
        "# display table schema and data\r\n",
        "df_gold_sales.printSchema()\r\n",
        "df_gold_sales.show()"
      ],
      "outputs": [
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
      },
      "source": [
        "# create calendar table for gold layer\r\n",
        "from datetime import date\r\n",
        "import pandas as pd\r\n",
        "from pyspark.sql.functions import to_date, year, month, dayofmonth, quarter, dayofweek, date_format\r\n",
        "\r\n",
        "# get first and last calendar date from sakes table \r\n",
        "first_sales_date = df_gold_sales.agg({\"Date\": \"min\"}).collect()[0][0]\r\n",
        "last_sales_date = df_gold_sales.agg({\"Date\": \"max\"}).collect()[0][0]\r\n",
        "\r\n",
        "# calculate start date and end date for calendar table\r\n",
        "start_date = date(first_sales_date.year, 1, 1)\r\n",
        "end_date = date(last_sales_date.year, 12, 31)\r\n",
        "\r\n",
        "# create pandas DataFrame with Date series column\r\n",
        "df_calendar_ps = pd.date_range(start_date, end_date, freq='D').to_frame()\r\n",
        "\r\n",
        "# convert pandas DataFrame to Spark DataFrame and add calculated calendar columns\r\n",
        "df_calendar_spark = (\r\n",
        "     spark.createDataFrame(df_calendar_ps)\r\n",
        "       .withColumnRenamed(\"0\", \"timestamp\")\r\n",
        "       .withColumn(\"Date\", to_date(col('timestamp')))\r\n",
        "       .withColumn(\"DateKey\", (year(col('timestamp'))*10000) + \r\n",
        "                              (month(col('timestamp'))*100) + \r\n",
        "                              (dayofmonth(col('timestamp')))   )\r\n",
        "       .withColumn(\"Year\", year(col('timestamp'))  )\r\n",
        "       .withColumn(\"Quarter\", date_format(col('timestamp'),\"yyyy-QQ\")  )\r\n",
        "       .withColumn(\"Month\", date_format(col('timestamp'),'yyyy-MM')  )\r\n",
        "       .withColumn(\"Day\", dayofmonth(col('timestamp'))  )\r\n",
        "       .withColumn(\"MonthInYear\", date_format(col('timestamp'),'MMMM')  )\r\n",
        "       .withColumn(\"MonthInYearSort\", month(col('timestamp'))  )\r\n",
        "       .withColumn(\"DayOfWeek\", date_format(col('timestamp'),'EEEE')  )\r\n",
        "       .withColumn(\"DayOfWeekSort\", dayofweek(col('timestamp')))\r\n",
        "       .drop('timestamp')\r\n",
        ")\r\n",
        "\r\n",
        "# write DataFrame to new gold layer table \r\n",
        "( df_calendar_spark.write\r\n",
        "                   .mode(\"overwrite\")\r\n",
        "                   .option(\"overwriteSchema\", \"True\")\r\n",
        "                   .format(\"delta\")\r\n",
        "                   .save(\"Tables/calendar\")\r\n",
        ")\r\n",
        "\r\n",
        "# display table schema and data\r\n",
        "df_calendar_spark.printSchema()\r\n",
        "df_calendar_spark.show()"
      ],
      "outputs": [
      ]
    }
  ]
}